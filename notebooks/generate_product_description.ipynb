{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd4d0db",
   "metadata": {},
   "source": [
    "# Generate Product Description - Text Generation\n",
    "<br>\n",
    "\n",
    "Before starting, please make sure this notebook is using **conda_python3** kernel from the top right!\n",
    "\n",
    "Run all the cells and inspect the output of each cell.\n",
    "\n",
    "<b> Please read the following instructions carefully! </b><br>\n",
    "\n",
    "Following instructions are common to ALL the notebooks you will run in this workshop.\n",
    "\n",
    "* You will use the Jupyter notebooks for understanding the code you will be implementing in the Cloud9 IDE. \n",
    "* These notebooks are not a part of the retail web application deployment process.\n",
    "* To save time and avoid any issues, we highly recommend you to run ALL cells and inspect output of each cell rather than running the cells individually.\n",
    "* Limit your experimentations within the notebook since we have a dedicated **Explore** section. \n",
    "* Within the notebooks, we have provided brief explanation of all the core concepts and terminologies you need for this workshop. \n",
    "    * If you are already aware of these concepts, feel free to skip reading the explanations.\n",
    "* We have provided many hyperlinks or URLs throughout the notebooks for the purpose of further learning. \n",
    "    * In the interest of time, we request you to open and read these URLs **after** today's workshop i.e., as **take home** materials, as much as possible. \n",
    "    * You can always find this notebook (and all other notebooks) in this [Github repository](https://github.com/aws-samples/retails-generative-ai-workshop/tree/main/notebooks) which contains these URLs. Feel free to bookmark the Github repo for later reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cca43",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, you will generate product description using Text-to-Text LLMs (Large Language Models) from Amazon Bedrock. Let's take a quick look into the two important concepts we will use in all of our notebooks. \n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction) is an opensource framework used for developing applications powered by LLMs (Large Language Models). LangChain provides many libraries, APIs and templates to make it easy for us to develop applications powered by LLMs. LangChain [integrates with Bedrock](https://python.langchain.com/docs/integrations/llms/bedrock), and can be used to invoke the Bedrock APIs. **Note:** We will be using LangChain for 4 out of the 6 features in this workshop. \n",
    "\n",
    "A prompt is natural language text that describes the task that an LLM should perform. For example, you may prompt the LLM: *Describe microbiomes* or *What is the abstract of this text?*. You can use prompts to do various tasks with LLM such as Text Generation, Text Summarization, Sentiment Analysis, Classification etc. [Here](https://www.promptingguide.ai/introduction/examples) are some additional prompting examples for later reference. \n",
    "\n",
    "To generate product description, we will create a prompt with a few input variables, and invoke Bedrock LLM using LangChain. \n",
    "\n",
    "![Text Generation](../images/text-generation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477242",
   "metadata": {},
   "source": [
    "### Install required dependencies\n",
    "\n",
    "The pip install command may take 1-2 minutes to run.\n",
    "\n",
    "**Important:** You may notice an **ERROR** and a warning that \"you may need to restart the kernel\" from the following cell. **Ignore** and proceed with the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --no-build-isolation --force-reinstall \\\n",
    "    \"boto3==1.28.63\" \\\n",
    "    \"awscli==1.29.63\" \\\n",
    "    \"botocore==1.31.63\" \\\n",
    "    \"langchain==0.0.309\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c8689",
   "metadata": {},
   "source": [
    "<h3> Import required packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c496d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b503ee9",
   "metadata": {},
   "source": [
    "<h3> Initialize Bedrock client </h3><br>\n",
    "\n",
    "We will use the [Boto client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock.html) for Bedrock to invoke the Bedrock APIs and call the LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb975aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b524f8",
   "metadata": {},
   "source": [
    "### Set inference parameters for LLM\n",
    "\n",
    "Every LLM has its own set of [inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html). Setting infence parameters values are optional (as they have defaults). \n",
    "\n",
    "Following are the inference parameters for [Claude Anthropic LLM](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-claude.html). Note that we will describe more about these parameters in the **Explore** section of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8026e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_modifier = {}\n",
    "inference_modifier['max_tokens_to_sample'] = 200\n",
    "inference_modifier['temperature'] = 0.5\n",
    "inference_modifier['top_k'] = 250\n",
    "inference_modifier['top_p'] = 1\n",
    "inference_modifier['stop_sequences'] = [\"\\n\\nHuman\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc02508",
   "metadata": {},
   "source": [
    "<h3> Initialize the LLM </h3>\n",
    "\n",
    "Using Langchain, initialize the **ClaudeInstant LLM** for text generation.\n",
    "    \n",
    "LangChain has abstracted away the Amazon Bedrock API and made it easy to build use cases. You can pass in your prompt and it is automatically routed to the appropriate API to generate the response. \n",
    "\n",
    "LangChain allows you to access Bedrock once you pass **boto3_bedrock** session information to LangChain. If you pass None as the boto3 session information to LangChain, LangChain tries to get session information from your environment. In order to ensure the right client is used we are going to instantiate one thanks to a utility method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af17ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing Anthropic Claude as model ID\n",
    "\n",
    "textgen_llm = Bedrock(\n",
    "            model_id=\"anthropic.claude-instant-v1\",\n",
    "            client=boto3_bedrock,\n",
    "            model_kwargs=inference_modifier,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb8c01",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "[Prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) offers an easy way for you to work with the prompts. In this case, we use a prompt template to pass in our input variables.  \n",
    "\n",
    "Let's create a prompt template with the input variables: product name, brand, category, colors available, details, and desired length of the output. Read the template carefully to understand the prompt. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "            input_variables=[\"brand\", \"colors\", \"category\", \"length\", \"name\",\"details\"], \n",
    "            template=\"\"\"\n",
    "            \n",
    "                Human: Create a catchy product description for a {category} from the brand {brand}. \n",
    "                Product name is {name}. \n",
    "                The number of words should be less than {length}. \n",
    "\n",
    "                Following are the product details:  \n",
    "\n",
    "                <product_details>\n",
    "                {details}\n",
    "                </product_details>\n",
    "\n",
    "                Briefly mention about all the available colors of the product.\n",
    "\n",
    "                Example: Available colors are Blue, Purple and Orange. \n",
    "\n",
    "                If the <available_colors> is empty, don't mention anything about the color of the product.\n",
    "\n",
    "                <available_colors>\n",
    "                {colors}\n",
    "                </available_colors>\n",
    "\n",
    "                Assistant:\n",
    "                \n",
    "                \"\"\"\n",
    "    \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb787594",
   "metadata": {},
   "source": [
    "### Provide sample details about a product\n",
    "\n",
    "Following are the input variables that will be passed to our prompt template. This prompt will be passed to the Bedrock LLM to generate product description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_brand = \"Legendaire\"\n",
    "product_category = \"Shirt\"\n",
    "product_name = \"Legendaire Shirt\"\n",
    "\n",
    "product_details = \"\"\"\n",
    "        - collared white shirt \n",
    "        - 80% cotton 20% polyester\n",
    "        - semi-casual\n",
    "        - great for office or golfing\n",
    "        - comfortable breathable material\n",
    "        - flex fit\n",
    "    \"\"\"\n",
    "\n",
    "product_colors = [\"White\", \"Black\", \"Blue\"]\n",
    "\n",
    "# Length of the desired product description (generated from LLM)\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c7503",
   "metadata": {},
   "source": [
    "Let's pass the above variables as inputs to the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(brand=product_brand, \n",
    "                                colors=product_colors,\n",
    "                                category=product_category,\n",
    "                                length=max_length,\n",
    "                                name=product_name,\n",
    "                                details=product_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f49a8",
   "metadata": {},
   "source": [
    "<h3> Call Bedrock with the constructed prompt </h3>\n",
    "\n",
    "Let's pass in the prompt and generate product description from Bedrock. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e41af",
   "metadata": {},
   "source": [
    "Get the second paragraph i.e, only the product description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b880850",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = response[response.index('\\n')+1:]\n",
    "print_ww(generated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd554f",
   "metadata": {},
   "source": [
    "### Amazon Titan Text\n",
    "\n",
    "We can also try to generate the product descripton using another Text-to-Text LLM from Bedrock. Let's use Amazon Titan LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87256f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Amazon Titan LLM from Bedrock\n",
    "textgen_llm = Bedrock(\n",
    "                model_id=\"amazon.titan-text-express-v1\",\n",
    "                client=boto3_bedrock)\n",
    "\n",
    "# Call the LLM with same prompt\n",
    "response = textgen_llm(prompt)\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee40fb",
   "metadata": {},
   "source": [
    "### You've successfully generated description for a product with Anthropic Claude LLM!\n",
    "\n",
    "Please stop the notebook kernel by selecting **Kernel -> Interrupt**.\n",
    "\n",
    "#### Now, let's integrate this feature into our retail web application. Please go back to Workshop Studio and follow the instructions to build this feature using your Cloud9 IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1c65ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
