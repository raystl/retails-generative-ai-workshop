{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b46b5f",
   "metadata": {},
   "source": [
    "<h1> Question Answering - Code Generation </h1>\n",
    "<br>\n",
    "\n",
    "Before starting, please make sure this notebook is using **conda_python3** kernel from the top right!\n",
    "\n",
    "Run all the cells and inspect the output of each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4a664",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, you will create a Q&A application using Bedrock. Following is the flow.\n",
    "\n",
    "1. Customer asks a question in natural language.\n",
    "2. Use Text-to-Text LLM to generate SQL query for the question. LLM will use the schema of the tables containing retail website data. This will be passed as one of the prompt input variables.\n",
    "3. Execute the query against the website data stored in Amazon RDS Postgres database.\n",
    "4. Use Text-to-Text LLM to interpret the query results in natural language.\n",
    "5. Display the answer to the question.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "![Code Generation](../images/sql-generation.png)\n",
    "\n",
    "**Note** that we use two layers of LLM here. \n",
    "\n",
    "First LLM invocation will be used to generate the SQL query. \\\n",
    "Second LLM invocation will be used to interpret the query results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcf4cd",
   "metadata": {},
   "source": [
    "### Install required dependencies\n",
    "\n",
    "**Important:** You may see an error or a warning that \"you may need to restart the kernel\" from the following cell. **Ignore** and proceed with the next cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --no-build-isolation --upgrade \\\n",
    "    \"boto3==1.28.63\" \\\n",
    "    \"awscli==1.29.63\" \\\n",
    "    \"botocore==1.31.63\" \\\n",
    "    \"langchain==0.0.309\" \\\n",
    "    \"psycopg2-binary==2.9.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97e23c",
   "metadata": {},
   "source": [
    "<h3> Import required packages </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import psycopg2\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5637d89",
   "metadata": {},
   "source": [
    "<h3> Initialize Bedrock client </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11248a45",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "We need to get the Postgres database schema we will be passing to the LLM. Typically, all the data from web applications are stored in a database. In our case, all the data from our re:Invent retails web application (like products, orders, customer reviews, product ratings etc.) are stored in RDS Postgres database. You can easily get the schema of the tables with the [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html) tool. \n",
    "\n",
    "We have already taken a dump of the table schemas from the RDS database and uploaded it to S3. Let's read this schema from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f3ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3_bucket = None\n",
    "response = s3.list_buckets()\n",
    "for bucket in response['Buckets']:\n",
    "    if 'reinvent-retails-bucket' in bucket['Name']: \n",
    "        s3_bucket = bucket['Name']\n",
    "\n",
    "s3_response = s3.get_object(Bucket=s3_bucket, Key=\"data/schema-mysql.sql\")\n",
    "schema = s3_response['Body'].read().decode(\"utf-8\")\n",
    "\n",
    "# Printing schemas of all the tables used in our web application\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44cbe3c",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "<p>Using this prompt template we are asking the LLM to generate an SQL query based on the Postgres database schema. Notice that we pass 7 rules for the LLM to adhere to while constructing the query. Read the prompt instructions carefully. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efefb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "            Human: Create an Postgres SQL query for a retail website to answer the question keeping the following rules in mind: \n",
    "            \n",
    "            1. Database is implemented in Postgres SQL.\n",
    "            2. Follow the Postgres syntax carefully while generating the query.\n",
    "            3. Enclose the query in <query></query>. \n",
    "            4. Use \"like\" and upper() for string comparison on both left hand side and right hand side of the expression. For example, if the query contains \"jackets\", use \"where upper(product_name) like upper('%jacket%')\". \n",
    "            5. If the question is generic, like \"where is mount everest\" or \"who went to the moon first\", then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.\n",
    "            6. If the question is not related to the schema, then do not generate any query in <query></query> and do not answer the question in any form. Instead, mention that the answer is not found in context.  \n",
    "            7. If the question is asked in a language other than English, convert the question into English before constructing the SQL query. The string and varchar fields stored in the database are always in English.  \n",
    "\n",
    "            <schema>\n",
    "                {schema}\n",
    "            </schema>\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Assistant:\n",
    "            \n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bfe3d",
   "metadata": {},
   "source": [
    "### Input variables\n",
    "\n",
    "Let's pass the natural language question and the schema as input variables to the prompt template we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88453155",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many products do you have in your store?\"\n",
    "\n",
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2225767",
   "metadata": {},
   "source": [
    "### Invoke LLM\n",
    "\n",
    "Now lets call the Bedrock LLM to generate SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Claude Anthropic LLM\n",
    "llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "\n",
    "# Pass question and postgres schema of the web application\n",
    "prompt = prompt_vars.format(question=question, schema=schema)\n",
    "\n",
    "# Print response\n",
    "llm_response = llm(prompt)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e6517",
   "metadata": {},
   "source": [
    "### Extract Generated Query\n",
    "\n",
    "As you can see from the above llm_response, the query is embedded inside **\\<query\\>\\<\\/query\\>** tags. Note that within the prompt template, we requested to enclose the LLM generated query in the *query* tags.\n",
    "\n",
    "Let's create a function retrieve the query inside the \\<query\\> tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c518b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_strings_recursive(test_str, tag):\n",
    "    # finding the index of the first occurrence of the opening tag\n",
    "    start_idx = test_str.find(\"<\" + tag + \">\")\n",
    " \n",
    "    # base case\n",
    "    if start_idx == -1:\n",
    "        return []\n",
    " \n",
    "    # extracting the string between the opening and closing tags\n",
    "    end_idx = test_str.find(\"</\" + tag + \">\", start_idx)\n",
    "    res = [test_str[start_idx+len(tag)+2:end_idx]]\n",
    " \n",
    "    # recursive call to extract strings after the current tag\n",
    "    res += extract_strings_recursive(test_str[end_idx+len(tag)+3:], tag)\n",
    " \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a46879",
   "metadata": {},
   "source": [
    "Getting the query using the extract_strings_recursive function we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a003149",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "\n",
    "if \"<query>\".upper() not in llm_response.upper():\n",
    "    is_query_generated  = False\n",
    "else:\n",
    "    is_query_generated  = True\n",
    "    query = extract_strings_recursive(llm_response, \"query\")[0]\n",
    "    print(\"Query generated by LLM: \\n\" +query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ccd331",
   "metadata": {},
   "source": [
    "### Execute the query\n",
    "\n",
    "Now, let's connect to the RDS database and run this LLM-generated query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize secrets manager\n",
    "secrets = boto3.client('secretsmanager')\n",
    "\n",
    "sm_response = secrets.get_secret_value(SecretId='postgresdb-secrets')\n",
    "\n",
    "database_secrets = json.loads(sm_response['SecretString'])\n",
    "\n",
    "# Get secrets from the secrets manager\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "dbname = database_secrets['name']\n",
    "\n",
    "# Connect to PostgreSQL database\n",
    "dbconn = psycopg2.connect(host=dbhost, user=dbuser, password=dbpass, port=dbport, database=dbname, connect_timeout=10)\n",
    "dbconn.set_session(autocommit=True)\n",
    "cursor = dbconn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297862cb",
   "metadata": {},
   "source": [
    "Executing the LLM-generated query in Amazon RDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the extracted query\n",
    "cursor.execute(query)\n",
    "\n",
    "query_result = cursor.fetchall()\n",
    "dbconn.close()\n",
    "\n",
    "resultset = ''\n",
    "if len(query_result) > 0:\n",
    "    for x in query_result:\n",
    "        resultset = resultset + ''.join(str(x)) + \"\\n\"\n",
    "\n",
    "print(\"Query result: \\n\" +resultset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b5251",
   "metadata": {},
   "source": [
    "### Interpret Results\n",
    "\n",
    "Now, lets create another prompt template to interpret the query results. \n",
    "\n",
    "This prompt template defines rules while describing query result. This is the final answer that will be seen by the user. Idea is to derive natural language answer for a natural language question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "        Human: This is a Q&A application. We need to answer questions asked by the customer at an e-commerce store. \n",
    "        The question asked by the customer is {question}\n",
    "        \n",
    "        We ran an SQL query in our database to get the following result. \n",
    "\n",
    "        <resultset>\n",
    "            {resultset}\n",
    "        </resultset>\n",
    "\n",
    "        Summarize the above result and answer the question asked by the customer keeping the following rules in mind: \n",
    "        \n",
    "        1. Don't make up answers if <resultset></resultset> is empty or none. Instead, answer that the item is not available based on the question.\n",
    "        2. Mask the PIIs phone, email and address if found the answer with \"<PII masked>\"\n",
    "        3. Don't say \"based on the output\" or \"based on the query\" or \"based on the question\" or something similar.  \n",
    "        4. Keep the answer concise. \n",
    "        5. Don't give an impression to the customer that a query was run. Instead, answer naturally. \n",
    "\n",
    "        Assistant:\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008cfaf",
   "metadata": {},
   "source": [
    "### Invoke LLM \n",
    "\n",
    "Let's pass the query result and the original question to this prompt template to interpret the results retrieved from the database in natural language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c07b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_vars = PromptTemplate(template=prompt_template, input_variables=[\"question\",\"resultset\"])\n",
    "\n",
    "# Pass question and result set to prompt template\n",
    "prompt = prompt_vars.format(question=question, resultset=resultset)\n",
    "\n",
    "# Call LLM to interpret query result\n",
    "describe_query_result = llm(prompt)\n",
    "\n",
    "print(\"Question asked by the user: \\n\")\n",
    "print(question + \"\\n\\n\")\n",
    "print(\"Answer interpreted by LLM: \\n\")\n",
    "print_ww(describe_query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d99689",
   "metadata": {},
   "source": [
    "<h3> You've successfully created a Q&A application using SQL generation with Amazon Bedrock!</h3>\n",
    "\n",
    "Please stop the notebook kernel by selecting **Kernel -> Interrupt**.\n",
    "\n",
    "#### Now, let's integrate this feature into our retail web application. Please go back to Workshop Studio and follow the instructions to build this feature using your Cloud9 IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9ad10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
